{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda:0'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.index = 0\n",
    "\n",
    "    def add_word(self, x):\n",
    "        if x not in self.word_to_index:\n",
    "            self.word_to_index[x] = self.index\n",
    "            self.index_to_word[self.index] = x\n",
    "            self.index += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextProcess(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dict = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        embedding = []\n",
    "        with open(path, \"r\") as texts:\n",
    "            for text in texts:\n",
    "                words = text.split() + [\"<end_line>\"]\n",
    "                for word in words:\n",
    "                    self.dict.add_word(word)\n",
    "                    embedding.append(self.dict.word_to_index[word])\n",
    "        texts.close()\n",
    "        embedding = torch.tensor(embedding).long()\n",
    "        n_batch = embedding.shape[0] // batch_size\n",
    "        embedding = embedding[: n_batch * batch_size]\n",
    "        embedding = embedding.view(batch_size, -1)\n",
    "        print(\"Embedding Completed!\")\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "epochs = 20\n",
    "batch = 20\n",
    "time_steps = 30\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = TextProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Completed!\n"
     ]
    }
   ],
   "source": [
    "rep = corpus.get_data(\"Data/train.txt\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([20, 1484])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5290"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = corpus.dict.__len__()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "49"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batch = rep.shape[1] // time_steps\n",
    "num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, vocal_size, embed_size, hidden_size, num_layers):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0) * out.size(1), out.size(2))\n",
    "        out = self.fc(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)\n",
    "    model = model.to(device)\n",
    "    Loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # Train the model.\n",
    "    for epoch in range(epochs):\n",
    "        # Initial state\n",
    "        a = torch.zeros(num_layers, batch, hidden_size)\n",
    "        a = a.to(device)\n",
    "        states = (a, a)\n",
    "        for i in range(0, rep.size(1) - time_steps, time_steps):\n",
    "            inputs = rep[:, i: i + time_steps]\n",
    "            targets = rep[:, i + 1: i + 1 + time_steps]\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            yh, _ = model(inputs, states)\n",
    "            loss = Loss(yh, targets.reshape(-1))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            step = (i + 1) // time_steps\n",
    "            if not step % 100:\n",
    "                print(\"Epoch [{}/{}], Loss: {:.4f}\".format(epoch + 1, epochs, loss.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    # Testing the model.\n",
    "    with torch.no_grad():\n",
    "        with open(\"Data/result.txt\", \"w\") as texts:\n",
    "            # Initial hidden states\n",
    "            a = torch.zeros(num_layers, 1, hidden_size)\n",
    "            a = a.to(device)\n",
    "            state = (a, a)\n",
    "            inputs = torch.randint(0, vocab_size, (1, )).long().unsqueeze(1)\n",
    "            inputs = inputs.to(device)\n",
    "            for i in range(500):\n",
    "                op, _ = model(inputs, state)\n",
    "                prob = op.exp()\n",
    "                word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "                inputs.fill_(word_id)\n",
    "                word = corpus.dict.index_to_word[word_id]\n",
    "                word = \"\\n\" if word == \"<end_line>\" else word + \" \"\n",
    "                texts.write(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdotd\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 8.5693\n",
      "Epoch [2/20], Loss: 5.9631\n",
      "Epoch [3/20], Loss: 5.2476\n",
      "Epoch [4/20], Loss: 4.7057\n",
      "Epoch [5/20], Loss: 4.1619\n",
      "Epoch [6/20], Loss: 3.7850\n",
      "Epoch [7/20], Loss: 3.3741\n",
      "Epoch [8/20], Loss: 2.9140\n",
      "Epoch [9/20], Loss: 2.5403\n",
      "Epoch [10/20], Loss: 2.2082\n",
      "Epoch [11/20], Loss: 1.8849\n",
      "Epoch [12/20], Loss: 1.6248\n",
      "Epoch [13/20], Loss: 1.2832\n",
      "Epoch [14/20], Loss: 1.0423\n",
      "Epoch [15/20], Loss: 0.7381\n",
      "Epoch [16/20], Loss: 0.4757\n",
      "Epoch [17/20], Loss: 0.2877\n",
      "Epoch [18/20], Loss: 0.1609\n",
      "Epoch [19/20], Loss: 0.1096\n",
      "Epoch [20/20], Loss: 0.0810\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "test_model(trained_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch GPU",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}